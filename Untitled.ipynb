{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "720f2212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1692 images belonging to 22 classes.\n",
      "106/106 [==============================] - 204s 2s/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mtcnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 143\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Step 1: Face Detection and Recognition\u001b[39;00m\n\u001b[0;32m    142\u001b[0m frame_face \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 143\u001b[0m boxes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmtcnn\u001b[49m\u001b[38;5;241m.\u001b[39mdetect(frame_face, landmarks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m boxes:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mtcnn' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "from scipy.spatial import distance as dist\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "\n",
    "# Face Recognition\n",
    "\n",
    "\n",
    "def vgg_face():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(2622, (1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "def preprocess_image(img):\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = img / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def face_recognition(roi, model):\n",
    "    roi = cv2.resize(roi, (224, 224))\n",
    "    roi = preprocess_image(roi)\n",
    "    embedding_vector = model.predict(roi)[0]\n",
    "    embedding_vector = scaler.transform(embedding_vector.reshape(1, -1))\n",
    "    embedding_vector_pca = pca.transform(embedding_vector)\n",
    "    result = clf.predict(embedding_vector_pca)[0]\n",
    "    y_predict = clf.predict_proba(embedding_vector_pca)[0]\n",
    "    return result, y_predict\n",
    "\n",
    "train_dir = '../large_files/Test_Dataset_Faces/'\n",
    "Train_Data = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    rescale=1/255.0,\n",
    ").flow_from_directory(train_dir, batch_size=16, subset=\"training\", target_size=(224, 224), shuffle=False)\n",
    "\n",
    "# Load face recognition models\n",
    "model = vgg_face()\n",
    "model.load_weights('../large_files/vgg_face_weights.h5')\n",
    "model = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "embedding_vector = model.predict(Train_Data, steps=len(Train_Data), verbose=1)\n",
    "y_train = Train_Data.labels\n",
    "\n",
    "scaler = StandardScaler()\n",
    "embedding_vector = scaler.fit_transform(embedding_vector)\n",
    "\n",
    "pca = PCA(n_components=128)\n",
    "embedding_vector = pca.fit_transform(embedding_vector)\n",
    "\n",
    "X_train, y_train = embedding_vector, y_train\n",
    "\n",
    "clf = SVC(kernel='linear', C=2., class_weight='balanced', decision_function_shape='ovo', probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Blink Pattern Detection and Authentication\n",
    "\n",
    "def get_ear(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Indices of left and right eyes in dlib's facial landmarks\n",
    "LEFT_EYE_INDICES = list(range(36, 42))\n",
    "RIGHT_EYE_INDICES = list(range(42, 48))\n",
    "\n",
    "EAR_THRESHOLD = 0.28  # Eye Aspect Ratio threshold for blink detection\n",
    "PAUSE_TIME_THRESHOLD = 2.0  # Pause time threshold (in seconds)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"../large_files/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "blink_count = 0\n",
    "blink_count_sequence = []\n",
    "prev_blink_detected = False\n",
    "blink_start_time = None\n",
    "\n",
    "# Two-Step Biometric Authentication\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Face Detection and Recognition\n",
    "    frame_face = frame.copy()\n",
    "    boxes, _ = mtcnn.detect(frame_face, landmarks=False)\n",
    "\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.astype(int)\n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "            # Check if ROI is empty\n",
    "            if roi.size == 0 or roi.shape[0] == 0 or roi.shape[1] == 0:\n",
    "                continue\n",
    "            result, y_predict = face_recognition(roi, model)\n",
    "\n",
    "            if len(result) > 1:\n",
    "                cv2.putText(frame, ImageClass(result[0]), (x1-5, y1-5), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "                cv2.putText(frame, str(np.round(y_predict[result[0]], 2)), (x2, y2-10), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "            elif len(result) == 0:\n",
    "                roi = cv2.cvtColor(roi, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(f'Pic{other}.png', roi)\n",
    "                cv2.putText(frame, 'Other', (x1-5, y1-5), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "            else:\n",
    "                cv2.putText(frame, ImageClass(result[0]), (x1-5, y1-5), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "                cv2.putText(frame, str(np.round(y_predict[result[0]], 2)), (x2, y2-10), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    # Step 2: Blink Pattern Detection and Authentication\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(gray, 0)\n",
    "    \n",
    "    for rect in rects:\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = np.array([[p.x, p.y] for p in shape.parts()])\n",
    "        left_eye = shape[LEFT_EYE_INDICES]\n",
    "        right_eye = shape[RIGHT_EYE_INDICES]\n",
    "        left_ear = get_ear(left_eye)\n",
    "        right_ear = get_ear(right_eye)\n",
    "        avg_ear = (left_ear + right_ear) / 2.0\n",
    "\n",
    "        blink_detected = avg_ear < EAR_THRESHOLD\n",
    "\n",
    "        if blink_detected and not prev_blink_detected:  # Blink start\n",
    "            blink_start_time = time.time()\n",
    "\n",
    "        if not blink_detected and prev_blink_detected:  # Blink end\n",
    "            blink_end_time = time.time()\n",
    "            blink_duration = blink_end_time - blink_start_time\n",
    "\n",
    "            if blink_duration >= PAUSE_TIME_THRESHOLD:\n",
    "                blink_count_sequence.append(blink_count)\n",
    "                blink_count = 0\n",
    "            else:\n",
    "                blink_count += 1\n",
    "\n",
    "        prev_blink_detected = blink_detected\n",
    "\n",
    "    # Check if authentication pattern is matched\n",
    "    AUTHENTICATION_PATTERN = [3, 2]  # Pattern of consecutive blinks for authentication\n",
    "    if blink_count_sequence == AUTHENTICATION_PATTERN:\n",
    "        print(\"Authenticated!\")\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e05463ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99630124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
